# I-SENSE Chatbot Assistant

## Overview
The I-SENSE platform is a tool used by various experts at the OCP Group for monitoring and analyzing industrial equipment. During initial use, training is provided to familiarize users with the platform's main functionalities. However, this training is not sufficient: experts frequently return with questions about how to use certain features or interpret data.

To address this recurring need and enhance user efficiency, the idea was proposed to develop a chatbot assistant capable of answering repetitive questions based on official documents and internal resources. The chatbot aims to provide fast, accurate, and reliable responses while maintaining a conversational context to enhance the user experience.

- **Objective**: Provide automated support to OCP experts through an intelligent chatbot based on internal documents.
- **Target Audience**: Engineers, analysts, and business experts using the I-SENSE platform.
- **Creation Date**: Created on September 6, 2025.

## Project Structure
The project is organized into three main files:

1. **`resume.md`**: File containing the knowledge base (internal documents).
2. **`build_vector.py`**: Script that vectorizes the content of `resume.md` for indexing and search.
3. **`app.py`**: Main application deployed with Streamlit, launching the chatbot and enabling user interaction.

## Installation

### Prerequisites
- Python 3.8 or higher.
- Internet connection to download Ollama and run Mistral.
- Ollama installed.
- Mistral model installed via Ollama.

## Installation Steps

### 1. Install Ollama

- **Windows**: [Download Ollama for Windows](https://ollama.com/download/windows)
- **macOS**: [Download Ollama for macOS](https://ollama.com/download/mac)

### 2. Install the Mistral Model

Open a command prompt or PowerShell and run:
```bash
ollama run mistral
```

### 3. Create a Virtual Environment

```bash
python -m venv venv
```

### 4. Activate the Virtual Environment

- **Windows**:
  ```bash
  .\venv\Scripts\activate
  ```
- **Linux / macOS**:
  ```bash
  source venv/bin/activate
  ```

### 5. Install Dependencies

```bash
pip install -r requirements.txt
```

### 6. Build the Vector Database

```bash
python build_vector.py
```

A `chroma_index` folder will be created, containing the vectorized data.

### 7. Launch the Application

```bash
streamlit run app.py
```

The application is now accessible via your browser.

## Detailed Instructions

#### `build_vector.py`

**Purpose**: Create a vector database from the `resume.md` file to enable efficient search of relevant passages by the chatbot.

**Functionality**:
- Loads the `resume.md` file.
- Splits the text into chunks to improve coherence during searches.
- Vectorizes these chunks using a model from the `sentence-transformers` family (default: `paraphrase-multilingual-MiniLM-L12-v2`, suitable for French).
- Saves the vector index in the `chroma_index` folder.

**Customization Options**:
- **Vectorization Model**: You can replace `paraphrase-multilingual-MiniLM-L12-v2` with another `sentence-transformers` model.
  > A larger model will provide better accuracy but will be slower and consume more memory.
- **Chunk Size** (default: `700`): Larger values include more context (useful for long, detailed responses).
- **Overlap** (default: `10`): Overlap between chunks to preserve continuity of ideas.

---

#### `app.py`

**Purpose**: Deploy a Streamlit interface for interacting with the chatbot.

**Functionality**:
- Loads the vector index created by `build_vector.py`.
- Uses the Mistral model via Ollama to generate responses.
- Provides a modern user interface with Streamlit, including:
  - Chat history
  - Reset button
  - Section displaying the sources used
- Uses two distinct prompts:
  - **QA_PROMPT**: Responds only based on documents (if no information is found → “I don’t know.”).
  - **SUMMARY_PROMPT**: Summarizes text in 2–3 clear sentences.

**Customization Options**:
- **Prompt**: Adjust the response style (e.g., more concise, detailed, or pedagogical).
- **Number of Sources** (`k=5`): Increase for more context, reduce for faster responses.
- **Maximum Display Length** (`1500` characters): Adjust based on desired detail level in the Sources section.
- **Model Temperature Parameter** (default: `0`):
  > `0` = precise and factual responses, `>0` = more creative but less rigorous.

## Expected Outputs

**On Screen**:
- Conversational responses generated by the Mistral model.
- Chat history displayed in the Streamlit interface.
- List of sources used for each response (excerpts from `resume.md`).

---

## Possible Errors

- **Ollama Not Running**:  
  Ensure the Ollama service is started (`ollama run mistral` at least once).

- **Virtual Environment Not Activated**:  
  Verify that your Python environment is activated (`.\venv\Scripts\activate` on Windows, `source venv/bin/activate` on Linux/macOS).

- **Missing `chroma_index` Folder**:  
  If the chatbot cannot find context, rerun:
  ```bash
  python build_vector.py
  ```

- **Streamlit Errors**:  
  Ensure all dependencies are installed (`pip install -r requirements.txt`).

---

## Academic Relevance

- **Innovation**:  
  The project applies Large Language Models (LLMs) to a specific industrial context (I-SENSE platform), automating user support.

- **Data Processing**:  
  Demonstrates the use of vectorization and Retrieval-Augmented Generation (RAG) to efficiently leverage internal documents.

- **Visualization and Usability**:  
  Integration into a Streamlit interface enhances user-friendliness and accessibility for non-technical experts.

- **Interdisciplinary Impact**:  
  Combines artificial intelligence, industrial engineering, and business analysis, opening prospects for innovation and industrial digitization.

---

## Future Improvements

- Add multilingual support to respond in French, English, or Arabic based on user preference.
- Implement automatic updates to the knowledge base (without rerunning `build_vector.py`).
- Extend the chatbot to other OCP internal documents (guides, procedures, technical manuals).
- Explore the possibility of integrating specialized models (e.g., LLaMA3, Mixtral) for comparison with Mistral.
- Add a feature to automatically export conversations to PDF or CSV.

---

## Contributors

Developed by:  
**Amcassou Hanane**

## Acknowledgments
I warmly thank my supervisors at OCP MS for their guidance and valuable advice throughout the project.

## License
All rights reserved.
